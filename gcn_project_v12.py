# -*- coding: utf-8 -*-
"""gcn_project_v12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19o230RTGEfviczRTHgEWZGJ8X1vGAuH1
"""

import tensorflow as tf
import torch
#from torch_geometric.data import Data
tf.version.VERSION

!pip install stellargraph
!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html
!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html
!pip install -q torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html
!pip install -q torch-geometric

!pip install grad-cam



"""# Graph generation and processing methods

"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy.sparse import csr_matrix
import networkx as nx

import time
from torchvision.datasets import VisionDataset
import stellargraph as sg
from torch_geometric.data import Data
from torch_geometric.utils.undirected import to_undirected

from collections import OrderedDict

from PIL import Image
import os
import os.path
import sys
from torchvision import transforms
from torch.utils.data import Dataset
from torchvision.models import alexnet
import re
import pandas as pd

from tqdm import tqdm

from stellargraph.mapper import PaddedGraphGenerator

import tensorflow_datasets as tfds
import zipfile
import requests
import torch
from torch_geometric.data import Data
from torch_geometric.utils.undirected import to_undirected

from collections import OrderedDict

from collections import OrderedDict
import torch.nn as nn

from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image
from torchvision.models import resnet50
import copy



import pandas as pd
import numpy as np

import stellargraph as sg
from stellargraph.mapper import PaddedGraphGenerator
from stellargraph.layer import GCNSupervisedGraphClassification
from stellargraph import StellarGraph
# 
from stellargraph import datasets

from sklearn import model_selection


from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.callbacks import EarlyStopping

import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
from collections import OrderedDict



from plot_utils import ipyDisplay
from graph_utils import estimate_graph_statistics
from graph_utils import GraphItem, ConnectedComponentCV2, GraphItemLogs
from image_utils import is_on, remove_isolated
from processing_utils import UnNormalize
from processing_utils import ToGraphTransform
from neural_nets import UNet


from io import BytesIO
import scipy.misc
import tensorflow as tf
from PIL import Image
import sys


import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import init

import os
import logging

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Subset, DataLoader
from torch.backends import cudnn

import torchvision
from torchvision import transforms
from torchvision.models import alexnet
import numpy as np
from PIL import Image
from tqdm import tqdm
import time
from sklearn.model_selection import train_test_split









ROOT_PATH = "/data_science/bioinformatics/GCN_project/vascular_segmentation"
ROOT_PATH = None
path ="./rccdataset/vascular_segmentation"
if os.path.exists(path):
  ROOT_PATH = path


# mean and standard deviation of colors in the RCC train dataset RGB convention
img_means = (0.7297678 , 0.4809845 , 0.65480953)
var = (0.02753073, 0.04772836, 0.02944909)

# for 512x512 images
graph_cc_mean = np.array([ 12.66008916 , 12.10070891, 110.54037857])
graph_cc_std = np.array([ 5.04678162  ,4.65336656 ,72.05804868])

img_std = np.sqrt(var)

gray_mean = (0.5709)
gray_std = np.sqrt(0.0433)


BATCH_SIZE = 4
RESIZE_DIM = 512
datasetManager = RCCDatasetManager(ROOT_PATH,
                resize_dim=RESIZE_DIM,
                download_dataset=True,
                standardize_config={'by_patient':False, 
                                     'by_patient_train_avg_stats_on_test':False,
                                     'by_single_img_stats_on_test':False}, 
                load_graphs=True,
                #img_color_mapping=cv2.IMREAD_COLOR,
                #seg_color_mapping=cv2.IMREAD_COLOR,
                                  verbose=True)

BATCH_SIZE = 4
augment_params_dict = {'resized_crop': None,#{'prob':1.0, 'original_kept_crop_percent':(0.7,1.0)},
                  'rotate' : None,#{'prob':1.0,'angle_range': 30},
                  'gauss_blur' : None,#{'prob':1.0,'kernel_size':3, 'sigma':(0.1, 1.5)},
                  'elastic_deform': {'alpha':(1,10), 'sigma':(0.07, 0.13), 'alpha_affine':(0.07, 0.13), 'random_state':None}
                    }
img_train_transform = transforms.Compose([
            transforms.ToTensor(),
            #transforms.Grayscale(),
            #transforms.Normalize(gray_mean, gray_std)
            #transforms.Normalize(img_means, img_std)
                               ] 
                             ) 
seg_train_transform = transforms.Compose([
            transforms.ToTensor(),
                               ] 
                             )

img_test_transform = transforms.Compose([
            transforms.ToTensor(),
            #transforms.Grayscale(),
            #transforms.Normalize(gray_mean, gray_std),
            #transforms.Normalize(img_means, img_std)
                               ] 
                             ) 
seg_test_transform = transforms.Compose([
            transforms.ToTensor(),
                               ] )
(train_dataset, validation_dataset, test_dataset), (train_gen, validation_gen, test_gen) = datasetManager.init_train_val_split(0.1, 
                                        batch_size=BATCH_SIZE,
                                        img_train_transform = img_train_transform,
                                        seg_train_transform = seg_train_transform,
                                        img_test_transform = img_test_transform,
                                        seg_test_transform = seg_test_transform,
                                        train_augment=True,
                                        **augment_params_dict
                                            )
 

train_dataloader = DataLoader( train_dataset , batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
val_dataloader = DataLoader(validation_dataset , batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
test_dataloader = DataLoader( test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)

"""# convnet training"""

from torchvision import models

vgg = models.vgg16_bn(num_classes= 2)

target_layer = vgg.features[40]

in_channels = 3
vgg.features[0] = nn.Conv2d(in_channels, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

from torchvision.models import resnet50

normalize = transforms.Normalize(gray_mean, gray_std)
normalize = transforms.Normalize(img_means, img_std)
img_train_transform = transforms.Compose([
            transforms.ToTensor(),
            #transforms.Grayscale(),
            #transforms.Normalize(gray_mean, gray_std)
            normalize,
                               ] 
                             ) 
seg_train_transform = transforms.Compose([
            transforms.ToTensor(),
                               ] 
                             )

img_test_transform = transforms.Compose([
            transforms.ToTensor(),
            #transforms.Grayscale(),
            #transforms.Normalize(gray_mean, gray_std),
            normalize,
                               ] 
                             ) 
seg_test_transform = transforms.Compose([
            transforms.ToTensor(),
                               ] )
experiment_mgr = ExperimentManager( datasetManager )
out_channels = 2
log_weights_path = './vgg_weights_path'
weights_filename = "vgg.pt"
model =  torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=False,num_classes=2)

#model = resnet50(pretrained=False, num_classes=2)
#model.fc = nn.Linear(in_features=2048, out_features=out_channels, bias=True)
#model = torchvision.models.resnet18()#torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=False)
#model.fc = nn.Linear(in_features=512, out_features=out_channels, bias=True)
augment_params_dict = {'resized_crop': None,#{'prob':1.0, 'original_kept_crop_percent':(0.7,1.0)},
                  'rotate' : {'prob':1.0,'angle_range': 30},
                  'gauss_blur' : None,#{'prob':1.0,'kernel_size':3, 'sigma':(0.1, 1.5)},
                  'elastic_deform': None#{'alpha':(1,4), 'sigma':(0.07, 0.13), 'alpha_affine':(0.07, 0.13), 'random_state':None}
                    }
loss_train, loss_validation, acc_train, acc_validation, test_accuracy, model = experiment_mgr.train_convnet(vgg, learning_rate=0.00001, 
                                          batch_size=4, img_train_transform=img_train_transform, seg_train_transform=seg_train_transform,
                                          img_test_transform=img_test_transform,
                                          seg_test_transform=seg_test_transform,
                                          epochs=400,
                                          log_weights_path=log_weights_path,
                                          weights_filename=weights_filename,
                                          augment=True, augment_params_dict=augment_params_dict)


fig = plt.figure(figsize=(12,4))
loss_fig = fig.add_subplot(121)
loss_fig.plot(loss_train)
loss_fig.plot(loss_validation)
loss_fig.set_title('model loss')
loss_fig.set_ylabel('loss')
loss_fig.set_xlabel('epoch')
loss_fig.legend(['train', 'validation'], loc='upper left')

acc_fig = fig.add_subplot(122)
acc_fig.plot(acc_train)
acc_fig.plot(acc_validation)
acc_fig.set_title('model accuracy')
acc_fig.set_ylabel('accuracy')
acc_fig.set_xlabel('epoch')
acc_fig.legend(['train', 'validation'], loc='upper left')

"""## gradcam check"""

BATCH_SIZE = 4
augment_params_dict = {'resized_crop': None,#{'prob':1.0, 'original_kept_crop_percent':(0.7,1.0)},
                  'rotate' : {'prob':1.0,'angle_range': 30},
                  'gauss_blur' : None,#{'prob':1.0,'kernel_size':3, 'sigma':(0.1, 1.5)},
                  'elastic_deform': None#{'alpha':(1,4), 'sigma':(0.07, 0.13), 'alpha_affine':(0.07, 0.13), 'random_state':None}
                    }
img_train_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Grayscale(),
            #transforms.Normalize(gray_mean, gray_std)
            normalize
            #transforms.Normalize(img_means, img_std)
                               ] 
                             ) 
seg_train_transform = transforms.Compose([
            transforms.ToTensor(),
                               ] 
                             )

img_test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Grayscale(),
            #transforms.Normalize(gray_mean, gray_std),
            #transforms.Normalize(img_means, img_std)
            normalize
                               ] 
                             ) 
seg_test_transform = transforms.Compose([
            transforms.ToTensor(),
                               ] )
(train_dataset, validation_dataset, test_dataset), (train_gen, validation_gen, test_gen) = datasetManager.init_train_val_split(0.1, 
                                        batch_size=BATCH_SIZE,
                                        img_train_transform = img_train_transform,
                                        seg_train_transform = seg_train_transform,
                                        img_test_transform = img_test_transform,
                                        seg_test_transform = seg_test_transform,
                                        train_augment=False,
                                            )
 

train_dataloader = DataLoader( train_dataset , batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
val_dataloader = DataLoader(validation_dataset , batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
test_dataloader = DataLoader( test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)  

train_dataset[0][0][2]

gradcam_logs, figures = test_classifier_gradcam(model, target_layer,val_dataloader, 
                             img_means, 
                             img_std,
                             gradcam_method= 'gradcam++')



"""
  from pytorch_grad_cam import GradCAM
  from pytorch_grad_cam.utils.image import show_cam_on_image
  from torchvision.models import resnet50

  #gradcam/gradcam++/scorecam
  dataloader = val_dataloader 
  standardization_img_means = img_means
  standardization_img_std = img_std
  gradcam_method= 'gradcam++'
  import matplotlib
  import matplotlib.pyplot as plt
  # non interactive backend for plotting images
  #curr_backend =  plt.rcParams['backend']
  matplotlib.use('agg')
  mapper_to_categorical = dataloader.dataset.dataset.mapping_id_to_label

  device = torch.device("cpu" if not torch.cuda.is_available() else "cuda")
  use_cuda = False

  if 'cuda' in str(device):
    use_cuda = True

  model.eval()
  target_layer = model.layer4[-1]
  cam = GradCAM(model=model, target_layer=target_layer, use_cuda=use_cuda)
  
  softmax = nn.Softmax(dim=1)
  figures = []
  gradcam_logs = []
  for (path_img, path_seg, img, seg, seg_gt), label in dataloader:
    img = img.float().to(device)
    label = label.long().to(device)
    y_preds = model(img)
   
    probabilities = softmax(y_preds)
    
    y_pred_binarized = probabilities.argmax(dim=1)
    
    corrects = torch.sum(y_pred_binarized == label).data.item() 


    
    for idx in range(len(img)):

      cuda_img = img[idx].unsqueeze(0)
 
      confidence_proba = probabilities[idx]
      #print(confidence_proba)
      ground_truth = label[idx]
      predicted =  confidence_proba.argmax().data.item()
      not_predicted = 1 - predicted

  
      correct = (label[idx] ==predicted)
 
      unnorm = UnNormalize(standardization_img_means, standardization_img_std)
      
      method = gradcam_method # Can be gradcam/gradcam++/scorecam

      input_tensor = img[idx].unsqueeze(0)# Create an input tensor image for the model
      rgb_img = unnorm(img[idx].cpu()).permute(1,2,0).numpy()
      
      grayscale_cam_predicted = cam(input_tensor=input_tensor, target_category=predicted)
      visualization_predicted = show_cam_on_image(rgb_img, grayscale_cam_predicted)

      grayscale_cam_not_predicted = cam(input_tensor=input_tensor, target_category=not_predicted)
      visualization_not_predicted = show_cam_on_image(rgb_img, grayscale_cam_not_predicted)

      if correct:
        correct_prediction_str = "correct"
      else:
        correct_prediction_str = "wrong"


      fig = plt.figure(figsize=(14,10))
      gradcam_predicted = fig.add_subplot(131)
      gradcam_predicted.imshow(visualization_predicted)
      gradcam_predicted.title.set_text('ROI of the prediction: ' + str(mapper_to_categorical[predicted]) + " probability=%.1f"%(confidence_proba[predicted]*100) \
                                       + "%\n" + correct_prediction_str + " prediction")
      raw = fig.add_subplot(132)
      raw.imshow(rgb_img)
      raw.title.set_text('Raw slide patch: '+ str(mapper_to_categorical[ground_truth]))
      gradcam_not_predicted = fig.add_subplot(133)
      gradcam_not_predicted.imshow(visualization_not_predicted)
      gradcam_not_predicted.title.set_text('ROI of the excluded prediction: '+ str(mapper_to_categorical[not_predicted]) )
      #fig.canvas.draw()
      #pil_img = Image.frombytes('RGB', fig.canvas.get_width_height(),fig.canvas.tostring_rgb())
      figures.append(fig)
      
      gradlog = GradcamLog(fig, mapper_to_categorical.copy(), confidence_proba.cpu().detach().numpy().squeeze()
                                  ,label[idx] )
      gradcam_logs.append(gradlog)
      
    del img
    del label
    """

target_layer = vgg.features[40]

from pytorch_grad_cam import *
    from pytorch_grad_cam.utils.image import show_cam_on_image
    from torchvision.models import resnet50

    #gradcam/gradcam++/scorecam
    dataloader = val_dataloader 
    standardization_img_means = img_means
    standardization_img_std = img_std
    gradcam_method= 'gradcam++'
    import matplotlib
    import matplotlib.pyplot as plt
    # non interactive backend for plotting images
    #curr_backend =  plt.rcParams['backend']
    matplotlib.use('agg')
    mapper_to_categorical = dataloader.dataset.dataset.mapping_id_to_label

    device = torch.device("cpu" if not torch.cuda.is_available() else "cuda")
    use_cuda = False

    if 'cuda' in str(device):
      use_cuda = True
    model = vgg
    model.eval()
    target_layer = target_layer#model.layer4[-1]
    cam = XGradCAM(model=model, target_layer=target_layer, use_cuda=use_cuda)
    #cam = GuidedBackpropReLUModel(model,use_cuda)
    softmax = nn.Softmax(dim=1)
    figures = []
    gradcam_logs = []
    (path_img, path_seg, img, seg, seg_gt), label = next(iter(dataloader))
    img = img.float().to(device)
    label = label.long().to(device)

y_preds = model(img)
   
    probabilities = softmax(y_preds)
    
    y_pred_binarized = probabilities.argmax(dim=1)
    
    corrects = torch.sum(y_pred_binarized == label).data.item()

cam = GradCAM(model=model, target_layer=target_layer, use_cuda=use_cuda)
      idx = 0
      cuda_img = img[idx].unsqueeze(0)
      
      confidence_proba = probabilities[idx]
      #print(confidence_proba)
      ground_truth = label[idx]
      predicted =  confidence_proba.argmax().data.item()
      not_predicted = 1 - predicted

      curr_mask = seg_gt[idx].squeeze()
  
      correct = (label[idx] ==predicted)
 
      unnorm = UnNormalize(standardization_img_means, standardization_img_std)
      
      method = gradcam_method # Can be gradcam/gradcam++/scorecam

      input_tensor = img[idx].unsqueeze(0)# Create an input tensor image for the model
      rgb_img = (unnorm(img[idx].cpu()).permute(1,2,0).numpy()/255.).clip(0.,1.)
      
      grayscale_cam_predicted = cam(input_tensor, target_category=predicted)# cam(input_tensor=input_tensor, target_category=predicted)
      visualization_predicted = show_cam_on_image(rgb_img, grayscale_cam_predicted)

      grayscale_cam_not_predicted = cam(input_tensor, target_category=predicted)#cam(input_tensor=input_tensor, target_category=not_predicted)
      visualization_not_predicted = show_cam_on_image(rgb_img, grayscale_cam_not_predicted)

      if correct:
        correct_prediction_str = "correct"
      else:
        correct_prediction_str = "wrong"


      fig = plt.figure(figsize=(14,10))
      gradcam_predicted = fig.add_subplot(131)
      gradcam_predicted.imshow(visualization_predicted)
      gradcam_predicted.imshow(curr_mask, cmap='jet',alpha=0.2)
      gradcam_predicted.title.set_text('ROI of the prediction: ' + str(mapper_to_categorical[predicted]) + " probability=%.1f"%(confidence_proba[predicted]*100) \
                                       + "%\n" + correct_prediction_str + " prediction")
      raw = fig.add_subplot(132)
      raw.imshow(rgb_img)
      raw.imshow(curr_mask, cmap='jet',alpha=0.2)
      raw.title.set_text('Raw slide patch: '+ str(mapper_to_categorical[ground_truth]))

from collections import Counter
Counter(rgb_img.flatten())

del unet
#visualization_predicted = show_cam_on_image(rgb_img, grayscale_cam_predicted)
print(grayscale_cam_predicted.min())
plt.imshow(grayscale_cam_predicted)

"""# segmentation training"""

experiment_mgr = ExperimentManager( datasetManager )
log_weights_path = "./unet_model_weight_path"
weights_filename = "unet.pt"

in_channels = 1
out_channels = 2
init_features = 32

img_train_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Grayscale(),
            transforms.Normalize(gray_mean, gray_std)
            #transforms.Normalize(img_means, img_std)
                               ] 
                             ) 
seg_train_transform = transforms.Compose([
            transforms.ToTensor(),
                               ] 
                             )

img_test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Grayscale(),
            transforms.Normalize(gray_mean, gray_std),
            #transforms.Normalize(img_means, img_std)
                               ] 
                             ) 
seg_test_transform = transforms.Compose([
            transforms.ToTensor(),
                               ] 
                             ) 
unet = UNet(in_channels=in_channels, out_channels=out_channels, init_features=init_features)

kwargs_dict = {'learning_rate':0.00005, 'batch_size':4, 'img_train_transform':img_train_transform, 'seg_train_transform':seg_train_transform,
                                          'img_test_transform':img_test_transform,
                                          'seg_test_transform':seg_test_transform,
                                          'epochs':10,
                                          'log_weights_path':log_weights_path,
                                          'weights_filename':weights_filename,
                                          'augment':True, 
                                          'augment_params_dict':augment_params_dict,
                                          'verbose_loss_acc': True}

loss_train, loss_validation, IOU_train, IOU_validation, IOU_test, model, segmentation_progress, segmentation_progress_pred, segmentation_progress_true_mask = experiment_mgr.train_unet(unet, **kwargs_dict)



"""# segmentation plotting"""

#epoch, items = segmentation_progress[7]
#idx, pred , true = items[0]
finals = []
pred_progress_list,mask_progress_list = [],[]
for epoch, items in segmentation_progress:
  cat_tensor = torch.cat(  [  torch.where( pred > 0., pred, true*0.5 )  for idx, pred , true in items if pred.shape[1] == 2058  ]    , dim=0)
  
  pred_cat = torch.cat(  [ pred for idx, pred , true in items if pred.shape[1] == 2058  ]    , dim=0)
  true_cat = torch.cat(  [ true for idx, pred , true in items if pred.shape[1] == 2058  ]    , dim=0)
  pred_progress_list.append(pred_cat)
  mask_progress_list.append(true_cat)
  finals.append(cat_tensor)
  #plt.figure(figsize=(10,10))
  #plt.imshow(final)

fig = plt.figure(figsize=(10,10))
plt.axis("off")

ims = [[plt.imshow(pred ,animated=True), plt.imshow(mask, animated=True, cmap='jet',alpha=0.2) ] for pred,mask in zip(pred_progress_list,mask_progress_list) ]

ims = ims + [ims[-1] for _ in range(5)]
ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=100000, blit=True)
writer = PillowWriter(fps=2)  
ani.save("seg.gif", writer=writer)  
HTML(ani.to_jshtml())

for idx, pred , true in items:
  print(pred.shape)

"""# torch gcn training"""

torch_train, train_labels = RCCDatasetManager.make_torch_graph_dataset(datasetManager.sample_dataset_graph_items,datasetManager.sample_dataset_graph_labels,
                                  loading_prompt_string=None)
torch_test, test_labels = RCCDatasetManager.make_torch_graph_dataset(datasetManager.out_of_sample_dataset_graph_items,datasetManager.out_of_sample_dataset_graph_labels,
                                  loading_prompt_string=None)

gcn = GCN(hidden_channels=64, num_node_features=3, num_classes=2, dropout=0.2)

best_model, train_acc_folds, val_acc_folds, test_acc_folds = experiment_mgr.train_torch_gcn( gcn, validation_size=0.1,    # all to be supplied as list of torch graphs and list of corresponding graph labels
                      train_torch_graphs=torch_train, train_graphs_labels=train_labels, val_torch_graphs=None, val_graphs_labels=None,  test_torch_graphs=torch_test, test_graphs_labels=test_labels,
                      cross_validation=True,
                      batch_size=32,
                      learning_rate=0.001,
                      epochs=200,
                      folds = 5,
                      n_repeats = 1,
                      verbose=True,verbose_epochs_accuracy=False)



"""# keras gcn training"""

experiment_mgr = ExperimentManager( datasetManager )
sg_train, train_labels = RCCDatasetManager.make_stellargraph_dataset(datasetManager.sample_dataset_graph_items,datasetManager.sample_dataset_graph_labels,
                                  loading_prompt_string=None)
sg_test, test_labels = RCCDatasetManager.make_stellargraph_dataset(datasetManager.out_of_sample_dataset_graph_items,datasetManager.out_of_sample_dataset_graph_labels,
                                  loading_prompt_string=None)


best_model, train_histories, train_acc_folds, val_acc_folds, test_acc_folds = experiment_mgr.train_sg_gcn( validation_size=0.1,    # all to be supplied as list of torch graphs and list of corresponding graph labels
                      train_sg_graphs=sg_train, train_graphs_labels=train_labels, val_sg_graphs=None, val_graphs_labels=None,  
                      test_sg_graphs=sg_test, test_graphs_labels=test_labels,
                      cross_validation=True,
                      batch_size=32,
                      learning_rate=0.0001,
                      epochs=800,
                      folds = 10,
                      n_repeats = 1,
                      verbose=True,verbose_epochs_accuracy=False)



"""# load predicted masks of the segmentation model as new dataset for gcn"""

experiment_mgr = ExperimentManager(datasetManager)
mask_pred_kwargs_dict = {'batch_size':8, 
                                          'img_train_transform':img_train_transform, 
                                          'seg_train_transform':seg_train_transform,
                                          'img_test_transform':img_test_transform,
                                          'seg_test_transform':seg_test_transform,
                                          'validation_split_size': 0.1}
(train_pred_graphs, train_pred_graph_labels), (val_pred_graphs, val_pred_graph_labels), (test_pred_graphs, test_pred_graph_labels) = \
                                    experiment_mgr.get_segmented_masks_graph_items(unet,  
                                      **mask_pred_kwargs_dict)

pred_segmentation_graph_dataset = train_pred_graphs + val_pred_graphs
pred_segmentation_graph_labels = train_pred_graph_labels + val_pred_graph_labels

# make stellargraph datasets
sg_train, train_labels = RCCDatasetManager.make_stellargraph_dataset(pred_segmentation_graph_dataset,  pred_segmentation_graph_labels,
                                  loading_prompt_string=None)
sg_test, test_labels = RCCDatasetManager.make_stellargraph_dataset(test_pred_graphs,  test_pred_graph_labels,
                                  loading_prompt_string=None)


best_model, train_histories, train_acc_folds, val_acc_folds, test_acc_folds = experiment_mgr.train_sg_gcn( validation_size=0.1,    # all to be supplied as list of torch graphs and list of corresponding graph labels
                      train_sg_graphs=sg_train, train_graphs_labels=train_labels, val_sg_graphs=None, val_graphs_labels=None,  
                      test_sg_graphs=sg_test, test_graphs_labels=test_labels,
                      cross_validation=True,
                      batch_size=32,
                      learning_rate=0.0001,
                      epochs=800,
                      folds = 10,
                      n_repeats = 1,
                      verbose=True,verbose_epochs_accuracy=False)

pred_segmentation_graph_dataset = train_pred_graphs + val_pred_graphs
pred_segmentation_graph_labels = train_pred_graph_labels + val_pred_graph_labels

# make torch graph datasets
sg_train, train_labels = RCCDatasetManager.make_torch_graph_dataset(pred_segmentation_graph_dataset,pred_segmentation_graph_labels,
                                  loading_prompt_string=None)
sg_test, test_labels = RCCDatasetManager.make_torch_graph_dataset(test_pred_graphs,test_pred_graph_labels,
                                  loading_prompt_string=None)

gcn = GCN(hidden_channels=64, dropout=0.2)

best_model, train_acc_folds, val_acc_folds, test_acc_folds = experiment_mgr.train_torch_gcn(gcn, validation_size=0.1,    # all to be supplied as list of torch graphs and list of corresponding graph labels
                      train_torch_graphs=sg_train, train_graphs_labels=train_labels, val_torch_graphs=None, val_graphs_labels=None,  
                      test_torch_graphs=sg_test, test_graphs_labels=test_labels,
                      cross_validation=True,
                      batch_size=32,
                      learning_rate=0.0001,
                      epochs=200,
                      folds = 10,
                      n_repeats = 1,
                      verbose=True,verbose_epochs_accuracy=False)

"""# crop segmentation experiment"""

img_train_transform = transforms.Compose([transforms.ToTensor(), 
                                          transforms.Grayscale(num_output_channels=1)])#,transforms.Normalize(gray_mean,gray_std)])
seg_train_transform = transforms.Compose([transforms.ToTensor()])
img_test_transform = transforms.Compose([transforms.ToTensor(), 
                                         transforms.Grayscale(num_output_channels=1)])#,transforms.Normalize(gray_mean, gray_std)])
seg_test_transform = transforms.Compose([transforms.ToTensor()])


log_weights_path = "./unet_weights"
in_channels = 1
out_channels = 2
init_features = 32
unet = UNet(in_channels=in_channels, out_channels=out_channels, init_features=init_features)
loss_train, loss_validation, IOU_train, IOU_validation, segmentation_progress, model =  ExperimentManager.train_crop_segmentation(path , unet, learning_rate=0.00001, epochs=20, crops_per_side = 4,
                         batch_size=4,
                        num_workers = 4,
                        img_train_transform=img_train_transform,
                        seg_train_transform=seg_train_transform,
                        img_test_transform=img_test_transform,
                        seg_test_transform=seg_test_transform,
                        log_weights_path="./log_dir",
                        weights_filename="fname.pt",
                        verbose=False,
                        verbose_loss_acc=True)

"""# ###########FINE#################"""



"""# crop dataset"""

def np_recompose_tensor(crops,  image_shape_getter_fn, n_channels_getter_fn,
                        paste_fn, init_output_fn ):
  coords, cropped_tensor = crops[0]
  crop_dim = image_shape_getter_fn(cropped_tensor)

  crop_len = crop_dim[0]

  max_idx_crop = 0
  for coords, cropped_tensor in crops:
    i,j = coords
    max_idx_crop = max(max(max_idx_crop, i),j)
  num_crops_per_side = max_idx_crop + 1
  original_side_len = num_crops_per_side * crop_dim[0]

  n_original_channels = n_channels_getter_fn(cropped_tensor)
  destination_tensor = init_output_fn(original_side_len,n_original_channels)  

  crop_idx = 0
  for i in range(num_crops_per_side):
    for j in range(num_crops_per_side):
        row_start = i*crop_len
        row_end = (i+1)*crop_len
        col_start = j*crop_len
        col_end = (j+1)*crop_len

        coords, curr_crop = crops[crop_idx]
        row_indices = slice(row_start,row_end)
        col_indices = slice(col_start,col_end)
        paste_fn(destination_tensor,row_indices, col_indices, curr_crop)
        #destination_tensor[:, row_start:row_end,  col_start:col_end ] = curr_crop

        crop_idx += 1
  return destination_tensor

def np_make_crops(tensor_img,image_shape_getter_fn,
                        crop_fn,split_side_in=4):
  crops = []

  img_shape = image_shape_getter_fn(tensor_img)
  # crop in squares (suppose tensor_img is square)
  crop_len = img_shape[0]//split_side_in

  for i in range(split_side_in):
    for j in range(split_side_in):
        row_start = i*crop_len
        row_end = (i+1)*crop_len
        col_start = j*crop_len
        col_end = (j+1)*crop_len

        #print(row_start,":", row_end)
        #print(col_start,":", col_end)
        row_indices = slice(row_start,row_end)
        col_indices = slice(col_start,col_end)
   
        tensor_crop = crop_fn(tensor_img,row_indices, col_indices)

        crops.append( ((i,j),tensor_crop)  )
  return crops
def img_paste_fn(destination_tensor,row_indices, col_indices, curr_crop):
              destination_tensor[row_indices,col_indices,:] = curr_crop
def seg_paste_fn(destination_tensor,row_indices, col_indices, curr_crop):
              destination_tensor[row_indices,col_indices] = curr_crop



# barely fits in colab high ram environment 
class SegmentationCroppedDatasetManager(RCCDatasetManager):
  def __init__(self,
                  num_crops_per_side=4, 
                    root_path = None,
                    verbose_loading=False):
        self.num_crops_per_side=num_crops_per_side
        self.root_path = root_path
        self.verbose_loading = verbose_loading


        self.load_graphs = False
        self.image_shape = 2048
        self.img_format='RGB'
        self.img_color_mapping=cv2.IMREAD_COLOR
        self.seg_color_mapping=cv2.IMREAD_COLOR
        self.single_crop_shape = self.image_shape//self.num_crops_per_side
        assert self.image_shape%self.num_crops_per_side == 0, "Error: provided number of crops per side results in non-homogeneous crop shapes."+\
                                                          f"\nRemaining crops on each side would have shape({self.image_shape%self.num_crops_per_side},{self.image_shape%self.num_crops_per_side})"+\
                                                          f"\n With original image of size ({self.image_shape}{self.image_shape}) and num crops per side={self.num_crops_per_side}"
          

        super().__init__( root_path=self.root_path,
                 in_memory=True,
                 load_graphs = False,
                resize_dim=self.image_shape,
                img_format='RGB',
                img_color_mapping=cv2.IMREAD_COLOR,
                seg_color_mapping=cv2.IMREAD_COLOR,
                verbose=self.verbose_loading)
        # dataset made of tuples (original_image_idx, crop)
        self.sample_cropped_images = []
        self.sample_cropped_masks = []
        # index used to reference, from  original_image_idx, to the data that allows to reconstruct the original image
        self.sample_cropped_images_index_data = []
        self.sample_cropped_masks_index_data = []
        sample_crop_index_num = 0


        self.sample_cropped_images, self.sample_cropped_masks, self.sample_cropped_images_index_data, self.sample_cropped_masks_index_data =\
                                     SegmentationCroppedDatasetManager.__load_crop_dataset(self.sample_dataset, self.num_crops_per_side)
        self.test_cropped_images, self.test_cropped_masks, self.test_cropped_images_index_data, self.test_cropped_masks_index_data =\
                                     SegmentationCroppedDatasetManager.__load_crop_dataset(self.out_of_sample_dataset, self.num_crops_per_side)
  
  @staticmethod
  def reconstruct(image_crops):
            (i,j), crop = image_crops[0]    
            n_dim = len(image_crops[0].shape)
            if torch.is_tensor(crop):
              if n_dim == 2:
                init_output_fn = lambda original_side_len,n_original_channels: torch.zeros( (original_side_len,original_side_len) ) 
              else:
                init_output_fn = lambda original_side_len,n_original_channels: torch.zeros( (n_original_channels, original_side_len, original_side_len) ) 
            else:
              if n_dim == 2:
                init_output_fn = lambda original_side_len,n_original_channels: torch.zeros( (original_side_len,original_side_len) ) 
              else:
                init_output_fn = lambda original_side_len,n_original_channels: torch.zeros( (original_side_len, original_side_len, n_original_channels) ) 

          
            recomposed_img = np_recompose_tensor(image_crops,  image_shape_getter_fn=lambda x: x.shape[:-1], n_channels_getter_fn= lambda x:x.shape[-1],
                                    paste_fn= img_paste_fn,  init_output_fn=init_output_fn )
            #recomposed_seg = np_recompose_tensor(result_seg,  image_shape_getter_fn=lambda x: x.shape, n_channels_getter_fn= lambda x:-1,
            # 
            #                       paste_fn= seg_paste_fn,  setter_output_dim_fn=lambda side_len, channels: (side_len,side_len))
            return recomposed_img

  @staticmethod
  def __load_crop_dataset(dataset, num_crops_per_side):
              # dataset made of tuples (original_image_idx, crop)
              cropped_images = []
              cropped_masks = []
              # index used to reference, from  original_image_idx, to the data that allows to reconstruct the original image
              cropped_images_index_data = []
              cropped_masks_index_data = []
              crop_index_num = 0
              for (img_path, seg_path, img, seg , seg_gt) , label  in dataset:
                  img_crops = np_make_crops(img,  image_shape_getter_fn=lambda x: x.shape[:-1],
                                          crop_fn=lambda tensor_img,row_indices, col_indices: tensor_img[row_indices,col_indices,:]  ,
                                        split_side_in=num_crops_per_side)
                  seg_gt_crops = np_make_crops(seg_gt,  image_shape_getter_fn=lambda x: x.shape,
                                          crop_fn=lambda tensor_img,row_indices, col_indices: tensor_img[row_indices,col_indices]  ,
                                          split_side_in=num_crops_per_side)
                  [cropped_images.append((crop_index_num,i,j, crop) )   for (i,j), crop in img_crops   ]
                  [cropped_masks.append((crop_index_num,i,j, crop))   for (i,j), crop in seg_gt_crops   ]
                  cropped_images_index_data.append(img_crops)
                  cropped_masks_index_data.append(seg_gt_crops)
                  crop_index_num += 1
              return cropped_images, cropped_masks, cropped_images_index_data, cropped_masks_index_data
# fits in memory but requires loading from disk every batch
class CropDataset(Dataset):
  def __init__(self,
                  root_path = None,
                 in_memory=True,
                 partition="Train",
                resize_dim=512,
                 num_crops_per_side =4,

                img_format ='RGB', # alternative is BGR, which is the standard format for cv2 but not other libraries
                img_color_mapping=cv2.IMREAD_COLOR,
                seg_color_mapping=cv2.IMREAD_COLOR,
                 img_transform=None,
                 target_transform=None,
               verbose=True):

        self.root_path = root_path
        self.in_memory = in_memory
        self.img_format = img_format
        self.resize_dim = resize_dim

        self.num_crops_per_side = num_crops_per_side

        self.img_color_mapping = img_color_mapping
        self.seg_color_mapping = seg_color_mapping
        self.img_transform = img_transform
        self.target_transform = target_transform
        self.verbose = verbose
        
        self.rccStorage = self.__load_rcc_dataset__(
                     partition=partition
                    )        
        
    
        self.X_img = self.rccStorage.X_img
        self.X_seg = self.rccStorage.X_seg
        self.y_ids = self.rccStorage.y_numeric
        self.y_labels = self.rccStorage.y_labels
        self.mapping_label_to_id = self.rccStorage.mapping_label_to_id
        self.mapping_id_to_label = self.rccStorage.mapping_id_to_label
        self.img_paths = self.rccStorage.img_paths
        self.seg_paths = self.rccStorage.seg_paths
  
        self.to_graph_transformer = ToGraphTransform(SQUARE_IMAGE_SIZE=self.resize_dim)

  def __getitem__(self, index):
        image_path = self.img_paths[index]
        seg_path = self.seg_paths[index]
        if self.in_memory:
            image = self.X_img[index]
            segmented = self.X_seg[index]
        else:
            image = read_image(image_path, self.resize_dim, self.img_color_mapping, self.img_format)
            segmented = read_image(seg_path, self.resize_dim, self.seg_color_mapping, self.img_format)

        segmented_ground_truth = get_segmentation_mask(segmented, self.seg_color_mapping, self.img_format ) 
        
        
        if self.img_transform is not None:
          image = self.img_transform(image)
        if self.target_transform is not None:
          segmented_ground_truth = self.target_transform(segmented_ground_truth)


        img_crops = np_make_crops(image,  image_shape_getter_fn=lambda x: x.shape[1:],
                                          crop_fn=lambda tensor_img,row_indices, col_indices: tensor_img[:, row_indices, col_indices]  ,
                                        split_side_in=self.num_crops_per_side)
        seg_gt_crops = np_make_crops(segmented_ground_truth,  image_shape_getter_fn=lambda x: x.shape[1:],
                                          crop_fn=lambda tensor_img,row_indices, col_indices: tensor_img[:, row_indices, col_indices]  ,
                                          split_side_in=self.num_crops_per_side)
        


        return  (img_crops, seg_gt_crops)

  def __len__(self):
        length = len(self.y_ids) 
        return length
  
  @staticmethod
  def reconstruct(image_crops):
            (i,j), crop = image_crops[0]    
            n_dim = len(image_crops[0].shape)
            if torch.is_tensor(crop):
              if n_dim == 2:
                init_output_fn = lambda original_side_len,n_original_channels: torch.zeros( (original_side_len,original_side_len) ) 
              else:
                init_output_fn = lambda original_side_len,n_original_channels: torch.zeros( (n_original_channels, original_side_len, original_side_len) ) 
            else:
              if n_dim == 2:
                init_output_fn = lambda original_side_len,n_original_channels: torch.zeros( (original_side_len,original_side_len) ) 
              else:
                init_output_fn = lambda original_side_len,n_original_channels: torch.zeros( (original_side_len, original_side_len, n_original_channels) ) 

          
            recomposed_img = np_recompose_tensor(image_crops,  image_shape_getter_fn=lambda x: x.shape[:-1], n_channels_getter_fn= lambda x:x.shape[-1],
                                    paste_fn= img_paste_fn,  init_output_fn=init_output_fn )

            return recomposed_img

        
  def __load_rcc_dataset__(self,
                     partition="Train",
                    ):
      
        labels_dict = dict()
        id_to_labels = []

        num_labels = 0
        X_img = []
        X_seg = []
        y_cancer_ids = []
        y_labels = []


        assert partition == RCCDatasetManager.X_TRAIN or partition == RCCDatasetManager.X_TEST, "Error: dataset split must either be 'Train' or 'Test' "
        
        
        folder_path = os.path.join(self.root_path, partition)

        img_paths = []
        seg_paths =[]

        # pattern required to identify the attributes of a given file
        # -sample type= segmentation or image
        # -id = expressed in x[0-9]_y[0-9]
        # -file type = {.png} is the only one considered but also .roi exist
        train_pattern = re.compile('([\w\W]+)([a-zA-Z][0-9]+_[a-zA-Z][0-9]+)\.([a-zA-Z0-9\.]+)$')
        test_pattern = re.compile('([\w\W]+_)([a-zA-Z0-9]+)\.([a-zA-Z0-9\.]+)$')



        replacement_string = None


        filename_pattern = None
        if partition == RCCDatasetManager.X_TEST :
            filename_pattern = test_pattern
        else:
            filename_pattern = train_pattern




        samples = []
        # load all labels first since they are not computationally expensive to process
        # then load all images if in_memory=True with tqdm progress bar
        # 
        
        folder_iterable = os.listdir(folder_path)
        if self.verbose:
            print("Scanning " + partition + " dataset directories")
            folder_iterable = tqdm(folder_iterable, leave=True,position=0 )
        for directory in folder_iterable:

            if directory not in labels_dict:
                    labels_dict[directory] = num_labels
                    id_to_labels.append(directory)
                    # pRCC or cRCC
                    category = directory
                    if partition == RCCDatasetManager.X_TEST:
                        replacement_string = "{}_{}_".format(category, "img")
                    else:
                        replacement_string = "{}_".format("crop")

                    curr_path = os.path.join(folder_path, directory)
                    counter = recursive_visit(curr_path,
                                                samples, 
                                                filename_pattern,
                                                replacement_string,0)
                    # all items that have been read are in the same folder that represents their category 
                    y_cancer_ids.extend([num_labels for _ in range(counter )])
                    y_labels.extend([category for _ in range(counter)])
                    num_labels += 1
                    
                    
        sample_iterable = samples
        if self.verbose:
            print("Loading " + partition + " dataset")
            sample_iterable = tqdm(samples, position=0, leave=True)
        
        for img_path, seg_path in sample_iterable:
            if self.in_memory:
                # if in memory, read img, and segmented sample and store the pair in the dataset
                img = read_image(img_path, self.resize_dim, self.img_color_mapping, self.img_format)#read_image(img_path, self.resize_dim, self.img_color_mapping)
                X_img.append(img)

                seg = read_image(seg_path, self.resize_dim, self.seg_color_mapping, self.img_format)
                X_seg.append(seg)

            # add paths to path lists
            img_paths.append(img_path)
            seg_paths.append(seg_path)
        rccStorage = RCCStorage( X_img, X_seg,
                                    y_labels, y_cancer_ids,
                                    id_to_labels, labels_dict,
                                    img_paths, seg_paths)      


        return rccStorage

BATCH_SIZE = 4
IMAGE_SIDE_LEN = 2048
CROPS_PER_SIDE = 4
VERBOSE = False
NUM_WORKERS = 4
img_train_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(img_means, img_std)])
seg_train_transform = transforms.Compose([transforms.ToTensor()])
img_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(img_means, img_std)])
seg_test_transform = transforms.Compose([transforms.ToTensor()])
train = CropDataset(root_path=path,in_memory=False,resize_dim=IMAGE_SIDE_LEN,
                            num_crops_per_side=CROPS_PER_SIDE,
                            partition="Train",
                            img_transform=img_train_transform,
                            target_transform=seg_train_transform,
                            verbose=VERBOSE)
test =  CropDataset(root_path=path,in_memory=False,resize_dim=IMAGE_SIDE_LEN,
                            partition="Test",
                            num_crops_per_side=CROPS_PER_SIDE,
                            img_transform=img_test_transform,
                            target_transform=seg_test_transform,
                    verbose=VERBOSE)
def collate(batch):
  cropped_images = []
  cropped_masks = []
  for img_crops, seg_crops in batch:
      for img_crop_data, seg_crop_data in zip(img_crops, seg_crops):
        (i,j), img_crop = img_crop_data
        (i,j), seg_crop = seg_crop_data
        cropped_images.append(img_crop.unsqueeze(dim=0))
        cropped_masks.append(seg_crop.unsqueeze(dim=0))
 
  x_img = torch.cat(cropped_images, dim=0)
  x_mask = torch.cat(cropped_masks, dim=0)
  batch_len = len(batch)
  l1 = [  x_img[idx:(idx+batch_len),...] for idx in range(0, x_img.shape[0], batch_len) ]
  l2 = [  x_mask[idx:(idx+batch_len),...] for idx in range(0, x_mask.shape[0], batch_len) ]
  return list(zip(l1,l2))#(x_img,x_mask)
train_crop_dataloader = DataLoader(train,batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)
test_crop_dataloader = DataLoader(test,batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate)

"""# crop train"""

import torchvision.utils as vutils
def train_crop_segmentation_model(log_weights_path,  train_dataloader, val_dataloader, model,
                              learning_rate=0.0001,
                             n_epochs=10,
                             verbose=True, verbose_loss_acc=False,
                             weights_filename="torch_weights.pt"):
    device = torch.device("cpu" if not torch.cuda.is_available() else "cuda")
    torch.cuda.empty_cache() 
    
    unet_weights_folder_path = log_weights_path
    weights_filename = weights_filename
    if not os.path.exists(unet_weights_folder_path):
      os.makedirs(unet_weights_folder_path)
    

    loaders = {"train": train_dataloader, "valid": val_dataloader}
    in_channels = 3
    criterion = nn.CrossEntropyLoss()


    out_channels=2
   
 
    model.to(device)
  

    log_dir = "./logs"
    if not os.path.exists(log_dir):
      os.makedirs(log_dir)



    
    best_validaton_BCE = sys.float_info.max
    #0.0001 goood
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    IOU_train = []
    IOU_validation = []
    loss_train = []
    loss_validation = []
    
    evaluation_multi_crop_batch = next(iter(val_dataloader))
    segmentation_progress = []
    step = 0
    epochs = n_epochs
    for epoch in tqdm(range(epochs), total=epochs,position=0, leave=True):
        loss_train_epoch = []
        loss_validation_epoch = []
        IOU_train_epoch = []
        IOU_validation_epoch = []
        for phase in ["train", "valid"]:
            if phase == "train":
                model.train()
            else:
                model.eval()

            

            for i, multi_crop_batch in enumerate(loaders[phase]):
                if phase == "train":
                    step += 1

                    loss_train_batch = []
                    loss_validation_batch = []
                    IOU_train_batch = []
                    IOU_validation_batch = []
                    for batch in multi_crop_batch:
                      img, mask = batch 
                      
                      img, mask  = img.to(device).float(), mask.long().squeeze(dim=1).to(device)
                      
                      optimizer.zero_grad()

                      with torch.set_grad_enabled(phase == "train"):
                          y_pred = model(img)
                        
                          loss = criterion(y_pred,mask)
                          
                          if phase == "valid":
                              softmax = nn.LogSoftmax(dim=1)
                              y_pred_binarized = softmax(y_pred).argmax(dim=1, keepdim=True)

                              curr_IOU = IoU(y_pred_binarized, mask)
                              loss_validation_epoch.append(loss.item())
                              IOU_validation_epoch.append(curr_IOU)
                              print("val iou:",curr_IOU)
                                      
                              loss_validation_batch.append(loss.item())
                              IOU_validation_batch.append(curr_IOU)

                          if phase == "train":
                              loss.backward()
                              optimizer.step()

                              softmax = nn.LogSoftmax(dim=1)
                              y_pred_binarized = softmax(y_pred).argmax(dim=1, keepdim=True)
                              
                              curr_IOU = IoU(y_pred_binarized, mask)

                              loss_train_epoch.append(loss.item())
                              IOU_train_epoch.append(curr_IOU)

                              loss_train_batch.append(loss.item())
                              IOU_train_batch.append(curr_IOU)
                              
                          del img
                          del mask
                          del y_pred


            if phase == "valid":
              progress = []
              for idx, batch in enumerate(evaluation_multi_crop_batch):
                    img, mask = batch 
                              
                    img, mask  = img.to(device).float(), mask.long().squeeze(dim=1).to(device)
          
                    with torch.set_grad_enabled(phase == "train"):
                                  y_pred = model(img)
                                  softmax = nn.LogSoftmax(dim=1)
                                  y_pred_binarized = softmax(y_pred).argmax(dim=1, keepdim=True)
                                  
                                  pred_grid = np.transpose(vutils.make_grid(
                                      y_pred_binarized.detach().float(), padding=2, nrow=5)
                                      .cpu(),(1,2,0))
                                  true_grid = np.transpose(vutils.make_grid(
                                      mask.unsqueeze(1).float(), padding=2, nrow=5)
                                      .cpu(),(1,2,0))
                                  progress.append( (idx, pred_grid, true_grid ) )
                      
                    del img
                    del mask
                    del y_pred
                    segmentation_progress.append( (epoch, progress)  )

  

            if phase == "train" :
                    loss_train.append(np.mean(loss_train_epoch))
                    IOU_train.append(np.mean(IOU_train_epoch))
                    
                    loss_train_epoch = []
                    IOU_train_epoch = []
            
            if phase == "valid":
                
                mean_validation_loss = np.mean(loss_validation_epoch)
                if mean_validation_loss < best_validaton_BCE:
                  best_validaton_BCE = mean_validation_loss
                  torch.save(model.state_dict(), os.path.join(unet_weights_folder_path, weights_filename))
                if verbose_loss_acc:
                  print("valid: ", mean_validation_loss)
                  print("iou_valid: ", np.mean(IOU_validation_epoch))
                loss_validation.append(mean_validation_loss)
                IOU_validation.append(np.mean(IOU_validation_epoch))

                loss_validation_epoch = []
                IOU_validation_epoch = []
    return loss_train, loss_validation, IOU_train, IOU_validation, model, segmentation_progress

log_weights_path = "./unet_weights"
in_channels = 3
out_channels = 2
init_features = 32
unet = UNet(in_channels=in_channels, out_channels=out_channels, init_features=init_features)

loss_train, loss_validation, IOU_train, IOU_validation, model,segmentation_progress = train_crop_segmentation_model(log_weights_path,  train_crop_dataloader, test_crop_dataloader, unet,
                              learning_rate=0.00001,
                             n_epochs=10,
                             verbose=True, verbose_loss_acc=True,
                             weights_filename="torch_weights.pt")

progress = segmentation_progress[0]
epoch_num, epoch_progress= progress 
for idx, pred_grid, true_grid in epoch_progress:
  plt.figure(figsize=(12,12))
  plt.imshow(pred_grid, cmap='gray' )
  plt.imshow(true_grid, cmap='jet', alpha=0.2)

y_pred = unet.cpu()(image)
softmax = nn.Softmax(dim=1)
y_pred_binarized = softmax(y_pred).argmax(dim=1, keepdim=True)

pred_grid = np.transpose(vutils.make_grid(
                                  y_pred_binarized.detach().float(), padding=100, nrow=5)
                                  .cpu(),(1,2,0)) 
true_grid = np.transpose(vutils.make_grid(
                                  mask.float(), padding=100, nrow=5)
                                  .cpu(),(1,2,0))
plt.figure(figsize=(12,12))
plt.imshow(pred_grid, cmap='gray' )
#plt.figure()
plt.imshow(true_grid, cmap='jet', alpha=0.2)
print

log_weights_path = "./log_weights"
    train_dataloader = train_crop_dataloader
    val_dataloader = test_crop_dataloader
    model = unet
    learning_rate=0.0001
    n_epochs=10
    verbose=True
    verbose_loss_acc=True

    weights_filename="torch_weights.pt"

    device = torch.device("cpu" if not torch.cuda.is_available() else "cuda")
    torch.cuda.empty_cache() 
    
    unet_weights_folder_path = log_weights_path
    weights_filename = weights_filename
    if not os.path.exists(unet_weights_folder_path):
      os.makedirs(unet_weights_folder_path)
    

    loaders = {"train": train_dataloader, "valid": val_dataloader}
    in_channels = 3
    criterion = nn.CrossEntropyLoss()


    out_channels=2
   
 
    model.to(device)
  

    log_dir = "./logs"
    if not os.path.exists(log_dir):
      os.makedirs(log_dir)



    
    best_validaton_BCE = sys.float_info.max
    #0.0001 goood
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    IOU_train = []
    IOU_validation = []
    loss_train = []
    loss_validation = []

    step = 0
    epochs = n_epochs
    for epoch in tqdm(range(epochs), total=epochs,position=0, leave=True):
        loss_train_epoch = []
        loss_validation_epoch = []
        IOU_train_epoch = []
        IOU_validation_epoch = []
        for phase in ["train", "valid"]:
            if phase == "train":
                model.train()
            else:
                model.eval()

            

            for i, multi_crop_batch in enumerate(loaders[phase]):
                if phase == "train":
                    step += 1

                    loss_train_batch = []
                    loss_validation_batch = []
                    IOU_train_batch = []
                    IOU_validation_batch = []
                    for batch in multi_crop_batch:
                      img, mask = batch 
                      
                      img, mask  = img.to(device).float(), mask.long().squeeze(dim=1).to(device)
                      
                      optimizer.zero_grad()

                      with torch.set_grad_enabled(phase == "train"):
                          y_pred = model(img)
                        
                          loss = criterion(y_pred,mask)
                          
                          if phase == "valid":
                              softmax = nn.LogSoftmax(dim=1)
                              y_pred_binarized = softmax(y_pred).argmax(dim=1, keepdim=True)

                              curr_IOU = IoU(y_pred_binarized, mask)
                              loss_validation_epoch.append(loss.item())
                              IOU_validation_epoch.append(curr_IOU)

                                      
                              loss_validation_batch.append(loss.item())
                              IOU_validation_batch.append(curr_IOU)

                              pred_grid = np.transpose(vutils.make_grid(
                                  y_pred_binarized.detach().float(), padding=100, nrow=5)
                                  .cpu(),(1,2,0))
                              true_grid = np.transpose(vutils.make_grid(
                                  mask.unsqueeze(1).float(), padding=100, nrow=5)
                                  .cpu(),(1,2,0))
                              
                              plt.figure(figsize=(12,12))
                              plt.imshow(pred_grid, cmap='gray')
                              plt.imshow(true_grid, cmap='jet', alpha=0.3)
                              plt.show()
                          if phase == "train":
                              loss.backward()
                              optimizer.step()

                              softmax = nn.LogSoftmax(dim=1)
                              y_pred_binarized = softmax(y_pred).argmax(dim=1, keepdim=True)
                              
                              curr_IOU = IoU(y_pred_binarized, mask)

                              loss_train_epoch.append(loss.item())
                              IOU_train_epoch.append(curr_IOU)

                              loss_train_batch.append(loss.item())
                              IOU_train_batch.append(curr_IOU)

                 
                              pred_grid = np.transpose(vutils.make_grid(
                                  y_pred_binarized.detach().float(), padding=100, nrow=5)
                                  .cpu(),(1,2,0)) 
                              true_grid = np.transpose(vutils.make_grid(
                                  mask.unsqueeze(1).float(), padding=100, nrow=5)
                                  .cpu(),(1,2,0) )
                              plt.figure(figsize=(12,12))
                              plt.imshow(pred_grid, cmap='gray' )
                              plt.imshow(true_grid, cmap='jet', alpha=0.3)
                              plt.show()
                              
                          del img
                          del mask
                          del y_pred

def testing(test_dataloader, net, criterion, img_means=None, img_std=None):
  if img_means is not None:
    unnormalizer = UnNormalize(img_means, img_std)

  net.eval()
  IoU_test =[]
  IOU_each_test_image = []
  figures = []
  for (img, seg, seg_gt),label in test_dataloader:
    seg = seg.to(device)
    img = img.to(device)
    seg_gt = seg_gt.long().to(device)
    seg_pred = unet(img)
    if isinstance(criterion, nn.BCEWithLogitsLoss ):
      y_pred_probabilities = nn.Sigmoid()(seg_pred)
      y_pred_binarized = torch.where(y_pred_probabilities >= 0.5, 1., 0.)
    else:
      log_softmax = nn.LogSoftmax(dim=1)
      y_pred_binarized = log_softmax(seg_pred).argmax(dim=1, keepdim=True)
    IoU_test.append(IoU(y_pred_binarized, seg_gt))

    N, c, h, w = img.shape

    for idx in range(N):    
      IOU_each_test_image.append( IoU(y_pred_binarized[idx], seg_gt[idx]) )
      fig = plt.figure(figsize=(30,24))
      if img_means is not None:
        curr_img = unnorm(img[idx])
      else:
        curr_img = img[idx]
      curr_img = curr_img.cpu().permute(1,2,0).numpy()
      curr_seg_gt = seg_gt[idx].cpu().squeeze()
      curr_pred = y_pred_binarized[idx].cpu().permute(1,2,0).numpy().squeeze(-1)

      raw = fig.add_subplot(141)
      raw.imshow( curr_img)

      raw_ground_truth_overlay = fig.add_subplot(142)
      raw_ground_truth_overlay.imshow( curr_img)
      raw_ground_truth_overlay.imshow(curr_seg_gt, cmap='jet', alpha=0.4)#cmap='gray')
    
      raw_pred_overlay = fig.add_subplot(143)
      raw_pred_overlay.imshow( curr_img)
      raw_pred_overlay.imshow(curr_pred, cmap='jet', alpha=0.4)

      ground_truth_pred_overlay = fig.add_subplot(144)
      ground_truth_pred_overlay.imshow( curr_seg_gt,cmap='gray')
      ground_truth_pred_overlay.imshow(curr_pred, cmap='jet', alpha=0.4)
      figures.append(fig)
    del seg
    del img
    del seg_gt
    del seg_pred
  return IOU_each_test_image,figures
IOU_test_images,figures = testing(test_dataloader, unet, nn.CrossEntropyLoss, img_means=img_means, img_std=img_std)











grayscale_cam_predicted = cam(input_tensor, target_category=predicted)

